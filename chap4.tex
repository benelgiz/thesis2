%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Methodology}

The aim of machine learning methods is to train a model with a given data set in order to predict the output values corresponding to a new input. 

\section{Machine Learning}

According to Arthur Samuel, name father of machine learning, "Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed". 
A rather more to the point but complicated definition offered by Tom Mitchell in 1998, quite new compared to Samuel\textquotesingle s in 1959, " A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E".

\subsection{Introduction}
Machine learning methods are mainly handled under two main categories: supervised and unsupervised learning as shown in Fig.~\ref{fig:machineLearningMethods}. 
In supervised learning, \textit{"right answer"} for each example in the data set is available to the user apriori. 
Whereas for unsupervised learning, the \textit{"right answer"} are not available. 
Although not as common as these methods, there are other machine learning algorithms serving specific purposes such as reinforcement learning and recommender systems. 
The common methodology in machine learning is that there are two phases, the learning and the prediction phases as in Fig.~\ref{fig:supervisedLearningBasics}. 

\begin{figure}
\begin{center}
\includegraphics[width=14cm]{figures/machineLearningMethods}    % The printed column width is 8.4 cm.
\caption{Common machine learning methodologies} 
\label{fig:machineLearningMethods}
\end{center}
\end{figure}

The first phase is comprised of learning from the available data to understand how the system behaves. 
In the second phase, the idea is to predict what will be the output of the system for a given input, depending on the knowledge about the system that you gained via learning phase. 

\begin{figure}
\begin{center}
\includegraphics[width=14cm]{figures/supervisedLearningBasics}    % The printed column width is 8.4 cm.
% For the old picture 
% \includegraphics[width=12cm]{figures/machineLearningBasics}   
\caption{Supervised learning basics } 
\label{fig:supervisedLearningBasics}
\end{center}
\end{figure}
 
The most common types of supervised leaning is the regression and classification problems. 
Since these are both of supervised learning types, the \textit{"right answer"} (right values of the output $y$) for each example is assumed to be known. 

\subsection{Terminology}

Since being clear about the terminology is essential, a basic introduction to mostly used terms is given in this section.  
Table~\ref{arm:machineLearningTerminology} shows representations of commonly used variables in machine learning. 
Although representations could differ from one reference to the other, throughout this thesis, one coherent representation given in Table~\ref{arm:machineLearningTerminology} is followed. 

\begin{table}
\caption{Machine learning terminology}
\label{arm:machineLearningTerminology}
\begin{center}
\begin{tabular}{||l|l||}\hline
x & input variable or feature \\\hline
y & output variable or target variable \\\hline
\textbf{x} & input variable vector or feature vector \\\hline
\textbf{y} & output variable vector or target variable vector \\\hline
m & number of training examples \\\hline
n & number of features \\\hline
i & index of training examples \\\hline
j & index of features \\\hline
$\textbf{x}_j^{(i)}$ & $i^{th}$ training example for feature $j$ \\\hline
$\textbf{y}^{(i)}$ & $i^{th}$ training output  \\\hline
$\textbf{h}_{\bm{\theta}}(\textbf{x})$ &hypothesis function\\\hline
$\bm{\theta}$ &parameter set\\\hline
$\textbf{J}({\bm{\theta}})$ &cost function  \\\hline

\end{tabular}
\end{center}
\end{table}

Many of the machine learning tools have defaults settings and easy to be used by beginners. 
Although they would most probably not achieve optimized results which would be the case in the hands of an experienced user, a beginner can easily start using those tools by sending input and output vectors as arguments to the machine learning functions. 

The configurations of input and output vectors to feed the learning algorithms might change from one to other but still there is a common convention that would be compatible with many. 
In this representation, each instance is given in different rows of the input vector. 
An example of predicting housing prices (taken from Andrew Ng's machine learning course) is given to show the way to constitute the input and output matrix. 
Table~\ref{arm:exampHousingPrices} uses a supervised learning regression example to show input and output vector representations. 
The aim of the problem in this example is to predict price of houses for given surface area. 
For that, known examples of houses with surface area and corresponding price have been given. 
Since the aim of the problem is to predict the price of a house given the surface area, price of house is the output of the problem. 
And to predict the price, the available information that is known to effect the price of a house is the surface area of the house, making it the input variable. 
So, in Table~\ref{arm:exampHousingPrices} each row corresponds to a different house, second column (surface area of house) constitute the input vector and the last row (price in 1000s) corresponds to the output vector. 

\begin{table}
\caption{Training set $(\textbf{x},\textbf{y})$ of housing prices - one-feature example}
\label{arm:exampHousingPrices}
\begin{center}
\begin{tabular}{ ||p{3cm}|p{3cm}|p{3cm}||}\hline
\textbf{training example index} $(i)$ & \textbf{Size in $feet^2$} ($\textbf{x}$) & \textbf{Price in $1000 \$ s$} ($\textbf{y}$) \\\hline
1 & 2104	   & 460 \\\hline
2 & 1416	   & 232 \\\hline
3 & 1534	   & 315 \\\hline
4 & 852	   & 178 \\\hline
$\vdots$ & $x^{(i)}$   & $y^{(i)}$ \\\hline
m & $x^{(m)}$   & $y^{(m)}$ \\\hline
\end{tabular}
\end{center}
\end{table}

In this problem, there is an assumption that the price of a house is only dependent on the surface area which would not hold in the real case. 
In real problems, it is more common that the output would depend not only one variable but more, or many. 
For that reason, the input vector in a realistic example would rather be an input matrix having different features in its columns as given in Table~\ref{arm:exampleMultiFeatures}. 
Here in this example, the output is still the price of house, but now the features which corresponds to each column of an input matrix (or sometimes called the feature matrix) are surface area, number of bedrooms, and can be enlarged until n features. 
It should be kept in mind the selection of features that would lead to a better prediction of the output is a challenging problem and usually requires experience on the system of interest. 

\begin{table}
\caption{Training set $(\textbf{x},\textbf{y})$ of housing prices - multi-feature example}
\label{arm:exampleMultiFeatures}
\begin{center}
\begin{tabular}{ ||p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}||}\hline
\textbf{training example index} $(i)$ & \textbf{Size in $feet^2$} ($\textbf{x}_1^{(i)}$) & \textbf{Number of bedrooms} ($\textbf{x}_2^{(i)}$) & \textbf{Feature j} ($\textbf{x}_j^{(i)}$) & \textbf{Feature n} ($\textbf{x}_n^{(i)}$) &\textbf{Price in $1000 \$ s$} ($\textbf{y}$) \\\hline
1 & 2104	& 5  & $\textbf{x}_j^{(1)}$ & $\textbf{x}_n^{(1)}$ & 460 \\\hline
2 & 1416 & 3 & $\textbf{x}_j^{(2)}$ & $\textbf{x}_n^{(2)}$ & 232 \\\hline
3 & 1534 & 3 & $\textbf{x}_j^{(3)}$ & $\textbf{x}_n^{(3)}$ & 315 \\\hline
4 & 852 & 2 & $\textbf{x}_j^{(4)}$ & $\textbf{x}_n^{(4)}$ & 178 \\\hline
$\vdots$ & $\textbf{x}_1^{(i)}$  & $\textbf{x}_2^{(i)}$  & $\textbf{x}_j^{(i)}$   & $\textbf{x}_n^{(i)}$ & $\textbf{y}^{(i)}$ \\\hline
m & $\textbf{x}_1^{(m)}$  & $\textbf{x}_2^{(m)}$  & $\textbf{x}_2j^{(m)}$   & $\textbf{x}_n^{(m)}$ & $\textbf{y}^{(m)}$ \\\hline
\end{tabular}
\end{center}
\end{table}


%\clearpage
%\newpage


\subsection{Steps towards the learning machine}

\subsubsection{Visualizing the data}

Having a preliminary glance at data might give the designer an idea about the ways to tackle the problem since it is the designer to select the features, the hypothesis, and the type of cost function. It is true that the algorithms are optimizing some parameters of the hypothesis and the cost function but still the structure itself is usually supplied by a human. 
Although there are tricks to select those in a better sense, machines still have a way to go towards Skynet.

Fig.~\ref{fig:housingPrices} shows the housing price depending on the surface area corresponding to the data given in Table~\ref{arm:exampHousingPrices}.
In this example, price of the houses are the output (target) variable given in y-axis and size (surface area in $feet^2$) is the feature (input) variable given in x-axis. 
Since output y is price which is a continuous value (thus not a label representing a class) this problem is called a regression problem. Plotting the data given shows that the problem could to be modeled by a linear hypothesis (a linear line to fit the data given). Then linear regression can be applied.

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{figures/linearRegressionExamp}    % The printed column width is 8.4 cm.
\caption{Regression example - Housing prices as a function of surface area of the house \cite{andrewNg_MachLearning}} 
\label{fig:housingPrices}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=5cm]{figures/classificationEx2}    % The printed column width is 8.4 cm.
\caption{Classification example} 
\label{fig:classificationEx2}
\end{center}
\end{figure}

Since an example for regression problem has been given, now a preliminary look at classification problem will be presented. 
Fig.~\ref{fig:classificationEx2} is a classification problem since the aim is to distinguish the class that a new input instance will belong to. 
Here the vertical dimension is not the output as in the previous example of linear regression but is another feature ($x_2$ in this example). 

And the information about the output is represented with the colors of the samples in the feature space.
Fig.~\ref{fig:classificationEx1} shows the output vs. inputs of a classification problem. 
This is just to show the difference between regression and classification problem figures. 
Usually in regression problems the y-axis represents the output. 
Here there is a direct analogy by representing a classification problem just as a regression problem, x-axis representing the input whereas y-axis representing the output. 
But usually in classification problems, value of y (which class it corresponds to) is represented with different colors as in Fig.~\ref{fig:classificationEx2} or with different signs such as O and X.

By just plotting the samples, in this example, it seems that logistic regression could yield satisfactory results since the data seems to be separated by a linear decision boundary. 

It is possible to choose the model structure that would represent the problem satisfactorily via visualizing the data set when the number of features are not large.
With an increasing number of feature set, determining the degree of the polynomial via visualizing data to represent the input output relationship could be more complicated. 
In that case, the user should refer to model selection techniques rather than the insights gained by data visualization. 


\begin{figure}
\begin{center}
\includegraphics[width=7cm]{figures/classificationEx1}    % The printed column width is 8.4 cm.
\caption{Classification example - output vs input} 
\label{fig:classificationEx1}
\end{center}
\end{figure}


In case the number of feature set is small but yet the visualization gives that the model for regression or classification problem can not be represented by a linear model (or linear decision boundary) feature mapping could be applied. 
Then linear regression or logistic regression (can only fit linear models unless features are mapped) could be applied to nonlinear systems with mapped features.  

\subsubsection{Feature Mapping}

This section applies to problems with smaller feature sets and to fit a nonlinear model with linear or logistic regression.
Depending on the results from visualizing data, it might be necessary to map the features since a straightforward implementation of linear / logistic regression will end up in linear model, linear decision boundary respectively. 
So for cases that the model or decision boundary being linear will not be enough to satisfactorily describe the training data, feature mapping should be applied.

The housing price prediction problem can be revisited here to give an example on feature mapping.
The data given in Table~\ref{arm:exampHousingPrices} and plotted in Fig.~\ref{fig:housingPrices} is a regression problem with one feature (surface area of house). 
To fit more accurately to the training data given, new features can be artificially added to the system as below, where the new features will be functions of the original feature.

\begin{align}
\label{eqn:costFuncExamp1}
\begin{split}
h_{\theta}(x) & = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3
\\
& = \theta_0 + \theta_1 (size) + \theta_2 {(size)}^2 + \theta_3 {(size)}^3
\end{split}
\end{align}

where

\begin{align}
\label{eqn:featureMapping1}
\begin{split}
x_1 & = size
\\
x_2 & = size^2
\\
x_3 & = size^3
\end{split}
\end{align}

\begin{figure}
\begin{center}
\includegraphics[width=13cm]{figures/classificationMicrochip}    % The printed column width is 8.4 cm.
\caption{Classification example - complex decision boundaries \cite{andrewNg_MachLearning}} 
\label{fig:classificationEx3}
\end{center}
\end{figure}

Another example that requires a nonlinear model to fit the data set is microchip assessment problem.
This example is also taken from the lecture notes of machine learning course by Andrew Ng. 
This is a two class classification problem, where one class of microchips is faulty while the other class is functional.
It is seen from the Fig.~\ref{fig:classificationEx3} that a linear decision boundary will not serve appropriately. 
So, in order to apply logistic regression for this problem, feature mapping is necessary where an example of new feature vector could be given as

\begin{equation}{\label{eqn:featureMapping2}}
\bm{x_{mapped}}
=\,
\begin{bmatrix}
1 & x_1 & x_2 & x_1^2 & x_1x_2 & x_2^2 & x_1^3 \cdots & x_1x_2^5 & x_2^6 
\end{bmatrix}
\,^ T
\end{equation} 

Let's take another example of binary (two-class) classification problem with 100 original features.
Since selecting only the original features, logistic regression can fit a linear decision boundary, so the features need to be mapped.
One way to map the feature set could be such that only the quadratic terms are taken into account, which would result in a new feature set such as
 
\begin{equation}{\label{eqn:featureMapping3}}
\bm{x_{mapped}}
=\,
\begin{bmatrix}
x_1^2 & x_1x_2 & x_1x_3 & \cdots & x_1x_{100} & x_2^2 & x_2x_3 & \cdots & x_3^2 & x_3x_4  \cdots  
\end{bmatrix}
\,^ T
\end{equation} 

with a complexity $O(n^2) \sim \frac{n^2}{2}$

If 3rd order terms are considered, the new feature set becomes

\begin{equation}{\label{eqn:featureMapping4}}
\bm{x_{mapped}}
=\,
\begin{bmatrix}
x_1^3 & x_1x_2x_3 & x_1^2x_2 & \cdots  & x_{11}x_{13}x_{17} & \cdots  
\end{bmatrix}
\,^ T
\end{equation} 

\begin{figure}
\begin{center}
\includegraphics[width=6cm]{figures/complexDecisionBoundary}    % The printed column width is 8.4 cm.
\caption{Classification example - complex decision boundaries} 
\label{fig:complexBoundary}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=14.1cm]{figures/ml_followChart}    % The printed column width is 8.4 cm.
\caption{Guide for feature mapping} 
\label{fig:ml_followChart}
\end{center}
\end{figure}


It is important to point out that, with a higher dimensional feature vector, the model will tend to fit more accurately to the training set but might be poor to generalize for a new input. 
This is called overfitting which is a common problem for many machine learning applications. 
More explanations about overfitting and ways to overcome it will be explained in regularization section. Increasing the dimensionality of the feature, the models might become more computationally expensive as well.

To fit a model or decision boundary of a circular or elliptical shape, only a subset of features can be considered such as

\begin{equation}{\label{eqn:featureMappin54}}
\bm{x_{mapped}}
=\,
\begin{bmatrix}
x_1^2 & x_2^2 & x_3^2 & \cdots & x_N^2  
\end{bmatrix}
\,^ T
\end{equation} 

Finally, when the number of features will be less, but it can not fit complex models or boundaries such as in Fig.~\ref{fig:complexBoundary}, then addition of higher terms such as given in 
Equ.~\ref{eqn:featureMapping3} or using Neural Networks should be considered.
A scheme to help select the strategy for mapping is given in  Fig.~\ref{fig:ml_followChart}. 

\subsubsection{Selecting Model Structure}

With a larger set of features, it might be difficult to have a sense of which model would be a better fit to the problem via visualizing the hypothesis. 
During the selection of the model or assessment of the hypothesis, it is not fair to evaluate the model using the same data set.   
Because in most cases, the model would fit very well with the data that it is trained with but not able to generalize to the new data which is the actual goal. 
The idea is to test on different data rather than the one that the model is fitted with. 
Assessment of the model with the data that the model is trained for will usually give better fit results but is likely to give worse results for a new data. 

Specifically in model structure selection problem, there is a new parameter to fit in addition to model parameter vector $\bm{\theta}$.
This parameter is the model structure number \emph{d}. \emph{d} can be the degree of the polynomial used as hypothesis, or could represent another hypothesis structure.
As for the case of training and evaluating with the same data, using only two data sets and calculating the test set error with the same data that is used for selecting the model, might lead to selecting a model biased towards the data that is used selecting the model.
So, the idea is to have different subsets of data to train the model (calculating $\theta$), to select the model structure and to evaluate the selected model structure and its corresponding parameter set. 
In order to do that, 3 distinct data sets are needed as shown in Fig.~\ref{fig:modelSelection}, to train parameters ($\theta$), to select the model structure ($d$), and associate an error value with that choices. 

Fig.~\ref{fig:modelSelection} summarizes the steps that should be followed which are explained in more detail below:

\begin{enumerate}
  \item Train the hypothesis for each model structure (d = 1,2,3 .. 10 in the example below is the degree of polynomial) by using the training set (\%60 of whole data, randomly selected) which will output the parameters of the hypothesis for the selected model.
  \item Then by using the cross validation data set (\%20 of whole data, randomly selected) calculate cross validation error for each model and select the hypothesis with the smallest cross validation error.

\begin{equation}{\label{eqn:costFuncCrossVal}}
J_{cv}(\theta)
=\,
\frac{1}{2m_{cv}} \sum\limits_{i=1}^{m_{cv}} \Big(h_\theta(x_{cv}^{(i)}) - (y_{cv}^{(i)})\Big)^2  
\end{equation} 

  \item Then by using the test set (\%20 of whole data, randomly selected) estimate generalization error for the selected hypothesis.
  
\begin{equation}{\label{eqn:costFuncTest}}
J_{test}(\theta)
=\,
\frac{1}{2m_{test}} \sum\limits_{i=1}^{m_{test}} \Big(h_\theta(x_{test}^{(i)}) - (y_{test}^{(i)})\Big)^2  
\end{equation} 

\end{enumerate}

\begin{landscape}
\begin{figure}
\begin{center}
%\includegraphics[width=16cm]{figures/modelSelection}    % The printed column width is 8.4 cm.
\includegraphics[width=23cm]{figures/modelSelectionNonVisualizableData}    % The printed column width is 8.4 cm.
\caption{Hypothesis selection guide} 
\label{fig:modelSelection}
\end{center}
\end{figure}
\end{landscape}

which can also be summarized as

\begin{alignat*}{6}
\label{eqn:exampCostFunc}
d &= 1, \quad h_{\theta}(x) \ && = \theta_0 + \theta_1 x\ \ && \longrightarrow \theta^{(1)}\ \ && \quad \longrightarrow J_{cv}(\theta^{(1)})\
\\
d &= 2, \quad h_{\theta}(x) \ && = \theta_0 + \theta_1 x + \theta_2 x^2 \ \ && \longrightarrow \theta^{(2)}\ \ && \quad \longrightarrow J_{cv}(\theta^{(2)})\
\\
d &= 3, \quad h_{\theta}(x) \ && = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3\ \ && \longrightarrow \theta^{(3)}\ \ && \quad \longrightarrow J_{cv}(\theta^{(3)})\
\\
& \ &&\ \vdots \ &&\ \ &&\ 
\\
d &= k, \quad h_{\theta}(x) \ && = \theta_0 + \theta_1 x_1 + \cdots+ \theta_k x^k\ \ && \longrightarrow \theta^{(k)}\ \ && \quad \longrightarrow J_{cv}(\theta^{(k)})\
\end{alignat*}

Select the d which makes $J_{cv}(\theta^{(k)})$ minimum and then estimate generalization error for the test set $J_{test}(\theta^{(4)})$.

\subsubsection{Scaling}

The next step is to normalize the data to make the values of features change within the same order of magnitude. 
This helps for calculation of parameters of the hypothesis via an optimization algorithm and its convergence rate.  
Although there are different methods for normalization, a common one is

\begin{equation}{\label{eqn:scalingFeatures}}
\bar{x}_j = \frac{x_j - \mu_j}{s_j} 
\end{equation} 

where the mean and range is given as

\begin{align}
\label{eqn:meandAndRange}
\begin{split}
\mu_j & = \frac{\sum\limits_{i=1}^m {x_j^i} }{m}
\\
s_j & = max(x_j) - min(x_j)
\end{split}
\end{align}

An important point is to keep that values $\mu_j, s_j$ and may be standard deviation in case it is preferred to range $s_j$. 
During prediction phase, the data first should be scaled with these values attained from learning data.

\subsubsection{Add Artificial Feature}

To write the hypothesis in vectorized form to accommodate larger feature sets and more complex hypothesis, feature vector is defined as

\begin{equation}{\label{eqn:featureVector}}
\vec{\bm{x}}
=\,
\begin{bmatrix}
x_0\quad x_1 \quad  x_2 \quad \cdots \quad x_n 
\end{bmatrix}
\,^T
\end{equation} 

where $x_0 = 1$. Fig.~\ref{fig:addArtificialFeature} shows the addition of the artificial feature in the feature matrix.

\begin{figure}
\begin{center}
\includegraphics[width=9cm]{figures/addArtificialFeature}    % The printed column width is 8.4 cm.
\caption{Adding an artificial feature of 1s.} 
\label{fig:addArtificialFeature}
\end{center}
\end{figure}

Also parameters of the hypothesis is given in vectorial form such as

\begin{equation}{\label{eqn:parameterVector}}
\vec{\bm{\theta}}
=\,
\begin{bmatrix}
\theta_0\quad \theta_1 \quad  \theta_2 \quad \cdots \quad \theta_n 
\end{bmatrix}
\,^T
\end{equation} 

So for the linear regression problem with a single feature where hypothesis is given as

\begin{equation}{\label{eqn:costFuncOneFeature}}
h_{\theta}(x) = \theta_0 + \theta_1 x_1
\end{equation} 

and also for $n$ features

\begin{equation}{\label{eqn:costFuncMltplFeature}}
h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots+ \theta_n x_n
\end{equation}

with the introduced vectorial form, the hypothesis can be denoted in its compact generic form such as 

\begin{equation}{\label{eqn:costFuncMltplFeature}}
h_{\theta}(x) = {\vec{\bm{\theta}}}^\intercal \vec{\bm{x}}
\end{equation}

\subsubsection{Selection of the Cost Function}

Cost function and the gradient are usually the inputs of the optimization algorithms to calculate the optimized $\theta$ values in order to fit a model or a decision boundary by using the data set given. 
The selection of cost function is another issue but for standard applications there are widely used cost functions for each type of machine learning (e.g linear regression, logistic regression). 
Examples of cost functions are given below. 

Cost function for Linear Regression 

\begin{equation}{\label{eqn:costFuncLinearRegression}}
J(\theta)
=\,
\frac{1}{2m} \sum\limits_{i=1}^{m} \Big(h_\theta(x^{(i)}) - (y^{(i)})\Big)^2  
\end{equation} 

Cost function for Logistic Regression (Classification) - a subcase of two class e.g $y \in \{0,1\}$ . 

\begin{equation}{\label{eqn:costFuncLogisticRegression}}
J(\theta)
=\,
\frac{1}{m} \sum\limits_{i=1}^{m} \Big[-y^{(i)}log(h_\theta(x^{(i)})) - (1-(y^{(i)}))log(1-h_\theta(x^{(i)}))\Big]
\end{equation} 

\subsubsection{Modify the Cost Function by Adding Regularization Term}

Regularization is done to avoid overfitting which is common for complicated models or decision boundaries such as having high order polynomial terms as below;

\begin{equation}{\label{eqn:featureMapping2}}
\bm{x_{mapped}}
=\,
\begin{bmatrix}
1 & x_1 & x_2 & x_1^2 & x_1x_2 & x_2^2 & x_1^3 \cdots & x_1x_2^5 & x_2^6 
\end{bmatrix}
\,^ T
\end{equation} 

Overfitting might end up fitting very well with the training set, but fail to generalize to the new data for the prediction phase. 
This means that the training set error is not a good predictor for how well the hypothesis would do on new examples.

\begin{figure}
\begin{center}
\includegraphics[width=16cm]{figures/underOverFit}    % The printed column width is 8.4 cm.
\caption{Underfitting, just right or overfitting examples - left to right} 
\label{fig:underOverFit}
\end{center}
\end{figure}

Fig.~\ref{fig:underOverFit} shows a system where it is easy to realize that the hypothesis is underfit, just right or overfit since there is only one feature (plot and see). 
But for problems with more features, it might not be that obvious or even not possible to plot and understand that the hypothesis is overfit or not. For that cases the standard way to evaluate the hypothesis
 
 \begin{enumerate}
 
  \item Split the data set to two parts (\%30 for the test set - \%70 for the training set). 
  Note that if the data set is randomly ordered than you can use the first \%70 for training and the rest for test, but if your data is not randomly ordered (data set is dependent on time or any kind of pattern in between the data set) \%70 - \%30 should be selected ramdomly.
  
  \item Learn parameters $\bm{\theta}$ from the new training set (\% 70 of the whole data set).

  \item Calculate the test set error by using test set (\%30 of whole data) with the parameters learnt by utilizing training set (\% 70 of the whole data).\\
  For linear regression the test set error is the cost function evaluated by the test set $(x_{test}(i), y_{test}(i))$ pairs and the parameters trained by utilizing the training set (\% 70 of the whole data)
	
	\begin{equation}{\label{eqn:costFuncTest}}
	J_{test}(\theta)
	=\,
	\frac{1}{2m_{test}} \sum\limits_{i=1}^{m_{test}} \Big(h_\theta(x_{test}^{(i)}) - (y_{test}^{(i)})\Big)^2  
	\end{equation} 
	
	For logistic regression, the test set error given as
	
	\begin{equation}{\label{eqn:costFuncLogisticRegressionModified}}
	J_{test}(\theta)
	=\,
	-\frac{1}{m_{test}} \sum\limits_{i=1}^{m_{test}} \Big[y_{test}^{(i)}\,log(\ h_\theta(\ x_{test}^{(i)})) + (1-(y_{test}^{(i)}))\,log(\ h_\theta(\ x_{test}^{(i)}))\Big]
	\end{equation} 
	
	or the misclassification error (0/1 misclassification error)
	
	\begin{equation}{\label{eqn:errorMisclassificationError}}
	 test\ error
	=\,
	\frac{1}{m_{test}} \sum\limits_{i=1}^{m_{test}} \Big(err(\ h_\theta(\ x_{test}^{(i)}), (y_{test}^{(i)})\Big)  
	\end{equation} 

	\begin{equation}{\label{eqn:misclassificationError}}
	  err(h_{theta}(x),y)=\begin{cases}
               1 \qquad if\ h_{\theta}\geq0.5,\ y=0 \ or\ if \ h_{\theta} < 0.5, \ y=1\\
               0 \qquad otherwise\\
            \end{cases}
	\end{equation} 

are used most commonly.
\end{enumerate}

Fig.~\ref{fig:overfittingRealization} visualizes and summarizes the procedure explained above.

The difference between calculating the test set error for overfitting (to distinguish if the model is overfit or not) and calculating the test set error for model structure selection is that the data set is split to 2 and 3 respectively since there is another parameter (\emph{d}) to select. 

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[width=17cm]{figures/overfittingRealization}    % The printed column width is 8.4 cm.
\caption{Procedure to detect overfitting} 
\label{fig:overfittingRealization}
\end{center}
\end{figure}
\end{landscape}


\textbf{Regularization}

To avoid overfitting by regularization, a term is added to cost function to penalize parameters $\vec{\bm{\theta}}$ for being large (i.e to make $\vec{\bm{\theta}}$ as small as possible), except ${\theta}_0$ (is not penalized by convention). Cost function for linear regression with the added term for regularization is given as

\begin{equation}{\label{eqn:costFuncRegularized}}
J(\theta)
=\,
\frac{1}{2m} \bigg[ \sum\limits_{i=1}^{m} \Big(h_\theta(x^{(i)}) - (y^{(i)})\Big)^2 +\lambda \sum\limits_{j=1}^{n} \theta_j^2 \bigg] 
\end{equation} 

For logistic regression the regularized cost function can be written as (for two class e.g $y \in \{0,1\}$)

\begin{equation}{\label{eqn:costFuncLogisticRegression}}
J(\theta)
=\,
\frac{1}{m} \sum\limits_{i=1}^{m} \Big[-y^{(i)}log(h_\theta(x^{(i)})) - (1-(y^{(i)}))log(1-h_\theta(x^{(i)}))\Big] +\frac{\lambda}{2m} \sum\limits_{j=1}^{n} \theta_j^2
\end{equation} 

Here, the challenge is to find the appropriate value for $\lambda$ which serves like a weight between the original part of the cost function and the second part, which penalizes theta vector for being larger. 
So it tends to decrease the values of $\theta$. But if this lambda is too big than it then the hypothesis converges to  

\begin{equation}{\label{eqn:costFuncRegularizedTooMuch}}
h_\theta(x)
=\,
\theta_0 
\end{equation} 

since all terms are penalized except $\theta_0$ (which is not penalized by convention) such that

\begin{equation}{\label{eqn:costFuncRegularizedTooMuchHow}}
h_\theta(x)
=\,
\theta_0 + \xcancel{\theta_1 x}  + \xcancel{\theta_2 x^2}  + \xcancel{\theta_2 x^3}  + \cdots + \xcancel{\theta_n x^n}
\end{equation} 

Thus the hypothesis becomes underfit for large values of $\lambda$ thus should be tuned to acquire the optimal value.

\subsubsection{Calculating parameters $\bm{\theta}$}

The cost function is minimized to calculate the best $\vec{\bm{\theta}}$ via changing $\vec{\bm{\theta}}$ with the use of optimization algorithms. To fit the parameters $\bm{\theta}$, we will try to find parameters $\bm{\theta}$ that minimize the cost function $J(\theta)$

\begin{equation}{\label{eqn:optimizationGoal}}
\underaccent{{\displaystyle \bm{\theta}}}{min} \ J(\bm{\theta})
\end{equation} 

For the case of a simplified one feature example, the minimization problem can be written as

\begin{equation}{\label{eqn:optimizationGoalSimplified}}
\underaccent{{\displaystyle \theta_0, \theta_1}}{min} \ J(\theta_0,\theta_1)
\end{equation} 

To achieve this minimization problem, there are a variety of algorithms having different properties. 
One of the easiest is gradient descent method which is selected to be mentioned here due to its simplicity.  Others include but not limited to conjugate gradient, BFGS, L-BFGS. 
Most of them requires the cost function and its gradient to minimize the cost function. 

\textbf{Gradient Descent}

The gradient descent algorithm will be presented for one feature example due to its simplicity for a linear regression model given in Equ.~\ref{eqn:linearRegressionTwoFeaturesSummary}

\begin{align}
\label{eqn:linearRegressionTwoFeaturesSummary}
\begin{split}
h_{\theta}(x) & = \theta_0 + \theta_1 x_1 
\\
J(\theta_1,\theta_2)
 & =\,
\frac{1}{2m} \sum\limits_{i=1}^{m} \Big(h_\theta(x^{(i)}) - (y^{(i)})\Big)^2  
\end{split}
\end{align}

The gradient descent algorithm for one feature and also for generic case are given below. 

 \begin{algorithm}
   \caption{Gradient Descent for one feature only}
    \begin{algorithmic}[1]
      \Function{GradDesOneFeat}{$X, y, theta\_init, alpha, num\_iters$}       
      
      \Comment{Inputs: X - training inputs, y - training outputs, theta - parameters,\\ alpha - learning rate, num\_iters - 		number of iterations(termination condition)}

        \State $theta0 = theta(1))$  
        \State $theta1 = theta(2))$  
        \State $m = length(y)$ \Comment Number of training examples
%        \State Let $L[1 \ldots {n_1} + 1]$ and $R[1 \ldots {n_2} + 1]$ be new arrays

        \For{$j = 1$ to ${iter\_num}$} \Comment Do until satisfied
                \For{$j = 1$ to ${m}$}     \Comment Do for all training examples
                         \State initialize each grad to zero
           	 	\State $grad0 \leftarrow grad0 + (costFunc(x) - y)$
		         \State $grad1 \leftarrow grad1 + (costFunc(x) - y) * x$
                    \EndFor
                    \State $theta0 \leftarrow theta0 - alpha * grad0$
                    \State $theta1 \leftarrow theta1 - alpha * grad1$
        \EndFor
       \EndFunction

\end{algorithmic}
\end{algorithm}
 
A conventional approach to assign initial value to $\theta$ is initialization by zero ($\theta_{init} = 0$).
It is important to know that for different initial values, $\theta$ might converge to different values (local minima but not the global one) as in Fig.~\ref{fig:localOrGlobalMinimaGD}. 
But for linear regression, it is shown that the cost function shown above is convex, meaning that it has no local minima but only one global minima. An example for cost function converging to different solutions depending on initialization is given in Fig.\ref{fig:localOrGlobalMinimaGD} (slightly different choice of $\theta_{init}$ might converge to the local minima, which is $J(\bm{\theta})$ smaller compared to surrounding regions, but does not converge to the smallest $J(\bm{\theta})$.

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{figures/localOrGlobalMinimaGD}    % The printed column width is 8.4 cm.
\caption{Gradient descent convergence dependance on $\theta_{init}$. Two different but close choice of $\theta_{init}$ might converge to local or global minima \cite{andrewNg_MachLearning}} 
\label{fig:localOrGlobalMinimaGD}
\end{center}
\end{figure}
 
Learning rate,  $\alpha$, gives the gradient descent its step size, so a bigger value means a faster convergence. 
But keep in mind for a bigger alpha, solution might not converge or even diverge while too small values might lead the optimization to converge very slowly.
A set of $\alpha$ should be tried and evaluated by their performances. A usual set to choose from would be $\alpha = [0.001, 0.01, 0.1, 0.003, 0.03, 0.3]$.
  
Number of iterations, $num\_iter$ is also a parameter to be chosen. 
As a start, conventionally it can be selected as $num\_iter = 1000$ and then, convergence of the cost function should be checked. 
If it has not yet converged but decreasing, a larger value for $num\_iter$ should be selected.


 \begin{algorithm}
   \caption{Gradient Descent}
    \begin{algorithmic}[1]
     \Function{GradDes}{$X, y, theta\_init, alpha, num\_iters$}    
           \State \textbf{Inputs:} X - training inputs, \\
           \qquad \qquad y - training outputs, \\
           \qquad \qquad theta\_init - initial guess for the optimized parameter theta,\\ 
           \qquad \qquad alpha - learning rate, \\
           \qquad \qquad num\_iters - number of iterations to find the optimized theta
           \State \textbf{Outputs:} theta - theta\_optimized/ theta vector that minimizes the cost function \\
           \qquad \qquad J - the values of the cost function calculated during the course of iterations
    \State repeat until convergence {
    \State $\theta_j \leftarrow \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1, \cdots)$
    	 \State For correct implementation, update simultaneously e.g
	 \State 	\qquad  $temp0 = \theta_0 - \alpha \frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1, \cdots)$
	 \State 	\qquad  $temp1 = \theta_1 - \alpha \frac{\partial}{\partial \theta_1}J(\theta_0, \theta_1, \cdots)$
          \State 	\qquad  \qquad $\vdots$
          \State 	\qquad  $\theta_0 = temp0$
	 \State 	\qquad  $\theta_1 = temp1$
	 \State 	\qquad  \qquad $\vdots$
		 }
       \EndFunction
\end{algorithmic}
\end{algorithm}

An important point in the algorithm is to update the $\bm{\theta}$ simultaneously. 
So during one iteration, the same $\bm{\theta}$ values should be substituted into $h_\theta(x)$, in order to calculate next values of $\theta_0, \theta_1$.
 
Convergence of gradient descent can be debugged by plotting the cost function as a function of iterations of the optimization.
In the preferred case, the plot should be decreasing, never increase, and converging to a steady value at the end of the optimization algorithm. 
Fig.~\ref{fig:visualizeCostFunc} shows two poor and one successful optimization by plotting the cost function versus iteration. 

Usually, it is a good practice to declare convergence when $J(\theta)$ decreases by less than $10^{-3}$ in one iteration. 
In case, gradient descent is not converging, a smaller $\alpha$ should be tried. 
But too small values of $\alpha$ could result in a slow convergence. 


\begin{figure}
\begin{center}
\includegraphics[width=15cm]{figures/visualizeCostFunc}    % The printed column width is 8.4 cm.
\caption{Evolution of $J(\theta)$ with respect to number of iterations. Left two figures showing that the optimization problem is not converging. The figure in the far right is the $J(\theta)$ evolution expected} 
\label{fig:visualizeCostFunc}
\end{center}
\end{figure}

\subsubsection{Visualizing error to diagnose over-fit/under-fit problems}

To further debug the hypothesis, a guide is given in Fig.~\ref{fig:debuggingHypothesis}. 
Here, the error for training and cross validation sets has to be calculated for a variety of polynomial degree or regularization parameter. 
When training and cross-validation errors are plotted as a function of polynomial degree (or complexity of hypothesis), a larger training and cross-validation error point toward a under-fit (high bias) hypothesis while a large cross-validation and low training error point an over-fit hypothesis (high variance) where training and cross validation errors are given as

\begin{equation}{\label{eqn:trainingError}}
J_{training}(\theta)
=\,
\frac{1}{2m} \sum\limits_{i=1}^{m} \Big(h_\theta(x^{(i)}) - y^{(i)}\Big)^2  
\end{equation} 

\begin{equation}{\label{eqn:crossvalError}}
J_{cv}(\theta)
=\,
\frac{1}{2m_{cv}} \sum\limits_{i=1}^{m_{cv}} \Big(h_\theta(x_{cv}^{(i)}) - (y_{cv}^{(i)})\Big)^2  
\end{equation} 

Here, it is assumed that that is no regularization term in the cost function and the training and cross validation errors are calculated in the same equation structure as the cost function itself (except evaluated on training and cross validation data). 
So the cost function for this case is

\begin{equation}{\label{eqn:costFuncLinearRegression}}
J(\theta)
=\,
\frac{1}{2m} \sum\limits_{i=1}^{m} \Big(h_\theta(x^{(i)}) - (y^{(i)})\Big)^2  
\end{equation} 

But in the case of a regularized cost function (the cost function having the regularization term) such as

\begin{equation}{\label{eqn:costFuncRegularized}}
J(\theta)
=\,
\frac{1}{2m} \bigg[ \sum\limits_{i=1}^{m} \Big(h_\theta(x^{(i)}) - (y^{(i)})\Big)^2 +\lambda \sum\limits_{j=1}^{n} \theta_j^2 \bigg] 
\end{equation} 

the training and cross validation errors are calculated ignoring the regularization term so they are defined exactly the same as given in Eq. \ref{eqn:trainingError} and Eq. \ref{eqn:crossvalError} respectively.

Another way to diagnose learning problems is to plot training and cross validation errors with respect to training set size, also named as the learning curves (2 rightmost curves in Fig.~\ref{fig:debuggingHypothesis}). 
Although normally the training set size is constant, here we artificially reduce the number of training examples and calculate the training and cross validation errors. 
For that, the model parameters $\bm{\theta}$ are fit to this reduced training set and then the training and cross validation errors are also calculated over this reduced training data set. 
In case the resulting curve gives a low training error and a high cross validation error for smaller training set sizes and also high training and cross validation errors for larger training sizes, the problem is most likely to be suffering from high bias (under-fit). 
Another typical signature of a high bias problem is that the training cross validation errors are similar in magnitude in addition to being high. 
If the learning curves give a low training error and a high cross validation error for the smaller training set size and also training error is increasing with training set size while cross validation error decreasing without leveling off, the problem is more likely to be suffering from high variance (overfitting). 
Another important signature for high variance problem is, the training error will be less then the cross validation error and the difference in between would remain significant. 

\subsubsection{Best practices}

After diagnosing the problem is an over-fit or under-fit, those practices is helpful to know. 

To fix a high bias (under-fit) model, it would help to try 

\begin{itemize}

\item{Adding features}
\item{Adding polynomial features}
\item{Decreasing the regularization term}

\end{itemize}

To fix a high variance (over-fit) model, it would help to try 

\begin{itemize}

\item{Adding more training examples}
\item{Decreasing the number of features}
\item{Increasing the regularization term}

\end{itemize}

\subsubsection{Prediction}

Finally, for a new set of input data, now output can be predicted using the model/classifier trained. 
One point to remember is that if scaling applied to training set, new data should be scaled as well before being fed to the model/classifier using the same mean and standard deviation values previously calculated from the training set. 
After that, $h(\theta)$ could be evaluated utilizing optimized $\theta$ and scaled new data set $x\_new\_scaled$. 
So $h_\theta(x\_new\_scaled)$ gives

\begin{equation}{\label{eqn:prediction}
h_{\theta}(x\_new\_scaled})
=\,
P\Big(y=1 \, | \, x; \, \theta \Big)
\end{equation} 

\begin{landscape}
\begin{figure}
\begin{center}
%\includegraphics[width=20cm,angle=90,origin=c]{figures/debuggingHypothesis}    % The printed column width is 8.4 cm.
\includegraphics[width=22cm]{figures/debuggingHypothesis}    % The printed column width is 8.4 cm.
\caption{Diagnosing machine learning problem by plotting training and cross-validation errors} 
\label{fig:debuggingHypothesis}
\end{center}
\end{figure}
\end{landscape}

which is the probability that $y = 1$ given $x\_new\_scaled$ parametrized by $\bm{\theta}$. 
But still it is important to keep in mind, some other learning methods, the output of the problem could differ, such as the output of an SVM (Support Vector Method) classifier would be directly the class label.



\section{Support Vector Machines}

\subsection{Introduction}

\iffalse Let $ E = \Big\{ \big( \vec{\bm{x}}_1, y_1 \big),\big( \vec{\bm{x}}_2, y_2, 
		\cdots, \big( \vec{\bm{x}}_m, y_m \big) \big) \Big\}$, 
		where $\vec{\bm{x}}_i \in {\rm I\!R}^n$ and $y_i \in \big\{0, 1\big\}$ 
		be a training example set. Assuming the training data linearly separable, 
\fi		

SVM is a relatively new approach for classification offering better generalization property thanks to its foundations on the structural risk minimization principle \cite{gunn1998support,yin2014study} while other classifiers usually only minimizes the empirical risk. This advances the capacity of generalization even with a small number of instances by reducing the risk of overfitting for a nicely tuned parameters setting. It can be applied to nonlinear systems and problems offering a vast number of features. Furthermore, taking advantage of convex optimization problems in the solution of SVM models, another attractive reason to use SVM rises as avoidance of global minimas, while Neural Networks is inherently prone to local minimas.

The idea behind SVM is to find an optimal hyperplane that will linearly separate the classes. This is achieved with the introduction of maximum margin concept which is the distance in between the boundaries when they are extended until hitting the first data point as in Fig.~\ref{fig:svmHyperplane}. The points closest to the hyperplane (decision boundary) are called the support vectors and are the representatives of the data sets to be used for the decision process. This helps to decrease the data to handle abruptly, enhancing the ability to cope with the curse of dimensionality and reducing the computational complexity.

SVM has other tricks to deal with not linearly separable problems such as using kernels to map data into higher dimensional feature spaces where they can be separated with a linear hyperplane.

\begin{figure}
\begin{center}
\includegraphics[width=9cm]{figures/svmHyperplane}    % The printed column width is 8.4 cm.
\caption{SVM working principle} 
\label{fig:svmHyperplane}
\end{center}
\end{figure}

A binary classifier is used in this work to classify two classes, faulty and nominal. The fault considered in this study is one of the control surface stuck at $0^{\circ}$. SVM being a supervised classification algorithm has two main phases as shown in Fig.~\ref{fig:supervisedLearning}. In the training phase, the model is learned as a fit to the labeled data that is fed to the SVM algorithm. This phase is usually followed with a tuning phase where some of the parameters of SVM is changed and results are compared to have the best fit via cross validation to avoid overfitting. The last phase is the prediction, where for a new instance the classifier predicts if it corresponds to a faulty or nominal condition.

Training data is comprised of labeled data where the label can belong to one of two possible cases. This data set is saved in $\bm{X} \in {\rm I\!R^{m \times n}}  $ where $m,n$ correspond to number of instances and features respectively. The label information corresponding to the measurement instances is also fed to the SVM algorithm during the training phase as output vector $\bm{y} \in \{-1,1\}$. The aim of SVM is to find an optimal hyperplane maximizing the margin by solving the optimization problem for non-linearly separable datasets

\begin{align}
min_{\gamma,\omega,b} \quad & \frac{1}{2} \norm{\omega}^2 + C \sum\limits_{i = 1}^m \xi_i \\
s.t. \quad & y^{i}(\omega^T x^(i) + b) \geq 1 - \xi_i, \ i = 1, \cdots, m\\
 & \xi_i \geq 0, \ i = 1, \cdots, m
% hic bir sey yazmazsan esitlikleri alt alta hizaliyor canim benim&=alo \\
% $ tek dolar arasi $ inline denklem
% $$ cift dolar arasi $$ satir atlayarak ortada denklem
\end{align}

To avoiding overfitting, which is the main problem of parametric discrimination approaches such as neural networks, parameter $C$ is tuned to result in the optimal fit for the cross validation set. The data set available is first divided to two portions with a percentage of \%20, \%80 where the bigger chunk is the training set and the remaining is the test set. Further, the training set is divided as cross-validation and training sets. The idea to split data is to avoid overfitting. Overfitting means that the models trained being very accurate fit for the data they are trained to but fail to generalize with new inputs resulting in bad prediction performance for the new data. To assess the performance of the classifier trained with the training data is tuned to give a better performance with the cross validation data. And then the final ability of the classifier is tested on the test set. This parameter also tuned for the outliers to generalize the distribution of the data rather than resulting in fine fits for each individual data in the training set. 
With a satisfactory result of the training \& tuning is followed by the prediction where the classifier predicts if the new measurement data belongs to the faulty or nominal class. The output of the SVM classification is not the probability that the new measurement belongs to one class as is in the traditional classification problems, but directly the class information it belongs to. For investigating the performance of the classifier on the test set, a method \cite{platt1999probabilistic} is used to calculate the posterior probabilities giving the probability that the new measurements belongs to faulty mode. Results shows as in  Fig.~\ref{fig:post_prob} that proper tuning achieves very accurate and instant detection for the drone fault.
 
\subsection{Application}

A binary classifier is used in this work to classify two classes, faulty and nominal. 
The faults considered in this study is the loss of effectiveness of the control surfaces and and the control surface stuck. 
Last section explains how the faulty data generated in flight and then labeled on the ground to be fed to the classifier. 
In this section, the classification of the faults will be explained in detail. 

SVM being a supervised classification algorithm has two main phases as shown in Fig.~\ref{fig:supervisedLearning}: training and prediction. 
The labeled data set is first divided to two portions with a percentage of \%20, \%80 where the bigger chunk is the training set and the remaining is the test set. 
Further, the training set is divided as cross-validation and training sets. The idea to split data is to avoid overfitting. 
Overfitting means that the models trained being very accurate fit for the data they are trained to but fail to generalize with new inputs resulting in bad prediction performance for the new data. 

\subsubsection{Training of the classifier}
In the training phase, the model is learned as a fit to the labeled data that is also an input to the SVM algorithm. 
Training data is comprised of labeled data where the label can belong to one of two possible cases. 
This data set is saved in $\bm{X} \in {\rm I\!R^{m \times n}}  $ where $m,n$ correspond to number of instances and number of features respectively. 
The label information corresponding to the measurement instances is also fed to the SVM algorithm during the training phase as output vector $\bm{y} \in \{-1,1\}$. 

The next step is to normalize the features of the data to make the values of features change with the same order of magnitude. 
The reason is due to its benefits to the calculation of parameters of the hypothesis via an optimization algorithm and its convergence rate.  
An important point is to keep that values $\mu_j, s_j$ and may be standard deviation if it is used instead of range $s_j$. 
During prediction phase, the data first should be scaled with these values attained from learning data.
If in the hypothesis artificial feature  $x_0 = 1$ is added to the features, do not apply scaling to the artificial feature, but this does not apply to SVM classification.

The aim of SVM is to find an optimal hyperplane maximizing the margin by solving the optimization problem for non-linearly separable datasets. 
SVM implements the idea of having a confidence in the prediction by using the concept of separating data with large margin. 
Some other classification methods, such as logistic regression, outputs the probability of a new instance's class, giving the confidence of the prediction as output inherently. 
On the other hand, SVM does only output if the new instance belongs to a class not necessarily pointing the confidence on this decision. 
Rather than including this confidence information as an output in terms of probabilities, this information is introduced with the functional and geometric margins. 
These definitions also serve for mathematically convenience, so that the optimization problem can be represented as a convex optimization problem.
Furthermore, introducing the Lagrange duality to obtain the dual form of the optimization problem, use of kernels, to work efficiently in higher dimensional spaces, is eased. 
The dual form also allows to utilize efficient optimization solvers such as Sequential Minimal Optimization (SMO)  \cite{platt1998sequential} which is the solver used in this work as well. Conventionally, training phase of SVM requires to solve a large quadratic programming (QP) problem. Especially for large training set, computational heaviness of this phase might limit the applicability of SVM to specific problems sets. To overcome this constraint, SMO breaks the QP problem into a series of smaller QP problems which can be solved analytically. Default settings for SVM binary classification, included in Matlab Statistics and Machine Learning Toolbox, utilizes SMO for optimization if outlier fractions has not been specified during the function call. 

%Tolerance for the gradient difference between upper and lower violators obtained by Sequential Minimal Optimization (SMO) or Iterative Single Data Algorithm (ISDA), specified as the comma-separated pair consisting of 'DeltaGradientTolerance' and a nonnegative scalar.
% The default values are: 1e-3 if the solver is SMO (for example, you set 'Solver','SMO')

Kernels are at the core of efficient SVM classifiers. A kernel, in general is defined as

\begin{equation}
K (x,z) = {\phi(x)}^T \phi(z)
\end{equation}

where $\phi$ represents the feature mapping. 
Default settings of Matlab's binary SVM classifier fits a linear model, which results in a linear decision boundary. 
If the system of interest requires a more complicated decision boundary to effectively classify the training data, mapping the original features might be necessary. 
Usually the original features of the systems are named as attributes while the mapped set are called the features. 
In other words, $\phi$ maps the attributes to the features. Kernels offer various elegant properties such as computational efficiency. 
Cleverly selected, SVM classifiers can learn in high dimensional spaces represented by $\phi$ without the need to explicitly find or represent $\phi$, but instead calculating $K(x,z)$, which might be computationally more efficient. 
In this study, Gaussian Kernel, which corresponds to an infinite dimensional feature mapping, is utilized to map the attributes.

\begin{equation}
K (x,z) = exp \bigg(-\frac{\Vert x - z \Vert ^ 2}{2 \sigma^2} \bigg)
\end{equation}

\subsubsection{Tuning of the classifier}
% HIGH VARIANCE = OVERFIT =   LARGE C = SMALL sigma^2
% HIGH BIAS = UNDERFIT = SMALL C =  LARGE sigma^2
% sigma^2 LARGE = HIGH BIAS 

Training phase is usually followed by a tuning phase where some of the parameters of SVM are tuned and results are compared in order to have the best fit via cross validation set. This is the phase where the parameters of the classifier are fixed to be used in the last phase, prediction.

\begin{figure}
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/objectiveFuncEval}    % The printed column width is 8.4 cm.
\caption{Convergence of the objective function} 
\label{fig:objectiveFuncEval}
\end{center}
\end{figure}

To avoiding overfitting, which is the main problem of parametric discrimination approaches such as neural networks, $C$ (box constraint) and the kernel scale (sigma), are tuned.
The classifier is trained with the training set and tuned via cross validation set, and then the selected classifier is evaluated with the test set. Cross-validation set selection of Matlab utilizes a random selection over the data set. 
To compare different methodologies for tuning and also the untuned classifiers, the script has been revised to generate random values from the same seed value to be consistent in comparisons.
Box constraint (C) and kernel scale (sigma) are tuned considering the presence of the outliers to generalize the distribution of the data rather than resulting in fine fits for each individual data in the training set. 

Two different ways are utilized to tune the classifier, heuristic and Bayesian optimization. In heuristic optimization, the strategy is to try a geometric sequence of the kernel scale (sigma parameter) scaled at the original kernel scale.  
Also a geometric sequence of the box constraint parameter, 11 values, from 1e-5 to 1e5 by a factor of 10 have been tried. Increasing box constraint might decrease the number of support vectors, but also might increase training time. 
Usually, increasing box constraint too much induces overfitting (high variance). 

\begin{figure}
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/objFuncModel}    % The printed column width is 8.4 cm.
\caption{Objective function values for different box parameter and sigma values} 
\label{fig:objFuncModel}
\end{center}
\end{figure}


Bayesian optimization tool from Matlab can be used in conjunction with the classification tool to optimize box constraint and the kernel scale. 
This tool outputs minimum objective value as a function of number of function evaluations (max. number of function evaluations can be changed by passing arguments to the function) as shown in Fig. ~\ref{fig:objectiveFuncEval}. 
Fig. ~\ref{fig:objFuncModel} shows the objective function values for a variety of box constraint and kernel scale values. 
The optimized values for box constraint and kernel scale in each iteration are also presented as a table as given in Fig. ~\ref{fig:optBayesianSteps}. 
Also a summary of Bayesian optimization showing the calculated and estimated best values for sigma and box constraint is given at the end of optimization such as Fig. ~\ref{fig:optimBayesianResults}.
With a satisfactory result of the training \& tuning is followed by the prediction where the classifier predicts if the new measurement data belongs to the faulty or nominal class. 

\begin{figure}
\begin{center}
\includegraphics[width=1.13\textwidth]{figures/optimizationSummaryStuckFault}    % The printed column width is 8.4 cm.
\caption{The optimized values for box constraint and kernel scale in each iteration and corresponding value of objective function} 
\label{fig:optBayesianSteps}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.93\textwidth]{figures/optimizationResultsStuckFault}    % The printed column width is 8.4 cm.
\caption{Final results for the optimization, time of execution, optimal values of box constraint and sigma values} 
\label{fig:optimBayesianResults}
\end{center}
\end{figure}

\subsubsection{Evaluating the classifier}
\label{evalClassifier}

The performance of the classifier is first quantified by \emph{loss of classification} as indicated with \emph{kFoldLoss} in this work. 
The training data set is  separated to 10 folds. For each fold, the \emph{loss of classification} computed for in-fold observations with a model trained on out-of-fold observations. 
Finally the \emph{loss of classification} is calculated as the \emph{classification error} averaged over all folds.
The idea to split and evaluate the performance of classifiers on different data sets is to avoid overfitting since the fitted classifier would give better results on the data set that it learnt from, but might not generalize well to new data. 
For that reason, both for evaluating the classifiers during tuning phase and finally evaluating the final classifier possessing the tuned parameters, were realized with different data sets.

Another means to evaluate performance of a classifier, available under Matlab Statistics and Machine Learning Toolbox, is via observing the \emph{classification edge}. 
The \emph{edge} is the weighted mean of the \emph{classification margins}. 
Here, the \emph{classification margin} for a binary classifier is defined, for each observation, as the difference between the \emph{classification score} for the \emph{true class} (faulty measurements in the considered problem) and the \emph{classification score} for the \emph{false class} (nominal measurements in the considered problem). 
In this definition, \emph{classification score} is considered as the signed distance from each observation to the decision boundary.

While all these variables would be satisfactory for performance analysis of classifiers, due to the skewed-class nature of the problem, another means of evaluations is necessary.
When the number of instances for different classes in a data set has a big difference, it is named as a skewed-class problem. 
The inherent trickiness to evaluate classifiers for such problems lies on the fact that, predicting only the more frequent class might lead to a misunderstanding that the classifier gives superior performance although it might not be even learning at all. 
For the problem of fault detection of a control surface would serve as a good example to clarify more. 
Since nominal data is not difficult to generate in real flight while it is difficult to fly faulty, the nominal data is much vast compared to the faulty data 

For such problems, in order to define a single metric to evaluate the abilities of classification, \emph{precision} and \emph{recall} should be defined.   
Fig.~\ref{fig:confusionMatrix} shows the confusion matrix which is used to calculate the precision and recall. 
Here, in general, the class indicated by 1 is the skewed class, corresponding to the fault class in this study. 
True positive refers to the number of instances that are predicted faulty and are actually faulty, false positive refers to the number of nominal instances which are miscalculated or predicted as faulty, false negative is the number of instances that are actually faulty but the classifier predicted that they are nominal, and finally the true negative is the number of instances that are predicted as nominal and are actually nominal.
Precision gives a measure on the fraction that was actually faulty of all the measurements that was predicted as faulty. 
Precision can be related to number of false alarms which is cautiously avoided in flight control systems. Precision is defined as 

\begin{figure}
\begin{center}
\includegraphics[width=0.5\textwidth]{figures/confusionMatrix}    % The printed column width is 8.4 cm.
\caption{Confusion matrix} 
\label{fig:confusionMatrix}
\end{center}
\end{figure}


\begin{equation}
precision = \frac{true \ positives}{true \ positives + false \ positives}
\end{equation}

Recall refers to the fraction of correctly detected faults of all the situations that was actually faulty. 
Recall can be related to the sensitivity of diagnostic systems. 
Actually an ideal health monitoring system is the one which accomplishes a reasonable balance between the false alarm rate and ability to detect reasonably small faults. 

\begin{equation}
recall = \frac{true \ positives}{true \ positives + false \ negatives}
\end{equation}

Finally as a single metric to evaluate the performance of the classification, F1 score is defined as

\begin{equation}
f1Score = \frac{2 * precision * recall}{precision + recall}
\end{equation}

F1 score, indicated as f1Score in the tables throughout this study is used as the main parameter to evaluate classifiers.

